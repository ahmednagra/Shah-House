import csv
import glob
import os

import openpyxl
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

from twisted.internet import defer
from twisted.internet import reactor

from spiders.aktialkv import AktialkvSpider
from spiders.blok import BlokSpider
from spiders.bo import BospiderSpider
from spiders.centrallkv import centrallkvSpider
from spiders.habita import HabitaSpider
from spiders.huoneistokeskus import HuoneistokeskusSpider
from spiders.kiinteistomaailma import KiinteistomaailmaSpider
from spiders.kotimeklarit import KotimeklaritSpider
from spiders.neliotliikkuu import NeliotliikkuuSpider
from spiders.nordlkv import NordlkvSpider
from spiders.opkoti import OpkotiSpider
from spiders.qvadrat import QvadratSpider
from spiders.remax import RemaxSpider
from spiders.rivehomes import RivehomesSpider
from spiders.roof import RoofSpider
from spiders.solidhouse import SolidhouseSpider
from spiders.sothebysrealty import SothebysrealtySpider
from spiders.westhouse import WesthouseSpider

from spiders.methods import comparison_excel


def start_sequentially(process: CrawlerProcess, crawlers: list):
    deferreds = []
    for crawler in crawlers:
        print('start crawler {}'.format(crawler.__name__))
        deferred = process.crawl(crawler)
        deferreds.append(deferred)

    return defer.DeferredList(deferreds)


def read_input_file():
    input_file = 'input/scraper_crawling_status.csv'

    with open(input_file, mode='r', encoding='utf8') as input_file:
        return list(csv.DictReader(input_file))


def read_output_files(crawlers):
    """
       Read output files generated by web crawlers.

       Args:
       - crawlers: List of crawler objects containing crawler names.

       Returns:
       - rows: List of dictionaries representing rows from the output files.
       """
    # Get all the output files
    output_files = glob.glob('output/properties/*.xlsx')
    rows = []

    for output_file in output_files:
        # Extract the spider name from the output file's name
        print(f"File {output_file.split('\\')[1]} is Reading")
        spider_name = os.path.basename(output_file).replace('.xlsx', '').split()[0]

        # Check if the spider is in the list of crawlers
        if spider_name in [crawler.name for crawler in crawlers]:
            try:
                workbook = openpyxl.load_workbook(output_file)
                sheet = workbook.active
                header = [cell.value for cell in sheet[1]]
                for row in sheet.iter_rows(min_row=2, values_only=True):
                    row_dict = dict(zip(header, row))
                    rows.append(row_dict)
            except Exception as e:
                print(f'{output_file} will raise the following error: {e}')
                return []

    return rows


def create_errors_file():
    """
    - when the scraper runs, it will create/overwrite the errors.txt file and then it will append all the errors
        from all spiders if there are any

    - Each time this file will have the latest run's errors if there are any

    """

    with open('ERRORS.txt', 'a') as f:
        pass


if __name__ == '__main__':
    scraper_status = read_input_file()
    create_errors_file()  # create a new file or overwrite the existing one

    # Don't run and process those spiders which are not set to True in the csv file
    dont_run_crawlers = [row['SpiderName'].strip() for row in scraper_status if
                         'true' not in row.get('Scrape', '').lower()]

    spider_crawlers = [
        AktialkvSpider, BlokSpider, BospiderSpider,
        centrallkvSpider, HabitaSpider, HuoneistokeskusSpider,
        KiinteistomaailmaSpider, KotimeklaritSpider, NeliotliikkuuSpider,
        NordlkvSpider, OpkotiSpider, QvadratSpider,
        RemaxSpider, RivehomesSpider, RoofSpider,
        SolidhouseSpider, SothebysrealtySpider, WesthouseSpider,
    ]

    crawlers = [crawler for crawler in spider_crawlers if crawler.name not in dont_run_crawlers]

    crawler_process = CrawlerProcess(get_project_settings())
    d = start_sequentially(crawler_process, crawlers)

    d.addBoth(lambda _: reactor.stop())
    reactor.run()

    output_files_read = read_output_files(crawlers)
    comparison_excel(data=output_files_read)
