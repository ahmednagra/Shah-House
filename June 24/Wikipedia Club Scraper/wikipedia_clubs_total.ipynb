{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_fetch_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google could not translate it!\n",
      "Google Translate\n"
     ]
    }
   ],
   "source": [
    "# fn_fetch_soup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "def fn_fetch_soup(url): \n",
    "    #print(url)\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))   \n",
    "# LANG\n",
    "    match = re.search(r'//(.*)\\.wikipedia', url)\n",
    "    lang = match.group(1) if match else None\n",
    "    if lang ==\"en\" or \"translate\" in url: \n",
    "\n",
    "        #session = requests.Session()\n",
    "        response = session.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    elif lang != \"en\":\n",
    "        url = f'https://translate.google.com/translate?sl=auto&tl=en&u={url}'\n",
    "        url = url.replace(\" \",\"_\")\n",
    "        #print(\"URL\", url)\n",
    "        global url_trans\n",
    "        url_trans = url\n",
    "        #print(url_trans)\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\") \n",
    "        # chrome_options.add_argument(r\"--user-data-dir=C:\\Users\\Marlon\\AppData\\Local\\Google\\Chrome\\User Data\\User 1\")\n",
    "        # chrome_options.add_argument(\"--profile-directory=Default\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url) \n",
    "        time.sleep(0.2)\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(lambda driver: driver.execute_script('return document.readyState') == 'complete')\n",
    "            #WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            #WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div#content')))\n",
    "            #WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, \"//div[@id='mw-content-text']//p\")))\n",
    "            time.sleep(1)\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            if \"Google Translate\" == soup.title.text:\n",
    "                print(\"Google could not translate it!\")\n",
    "            elif \"Wikipedia\" not in soup.title.text:\n",
    "                print(\"Translation maybe went wrong:\", soup.title.text)\n",
    "            #print(soup.title.text)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    return soup\n",
    "\n",
    "\n",
    "# #url = \"https://da.wikipedia.org/wiki/Aarhus_V\"\n",
    "# url = \"https://pt.wikipedia.org/wiki/Phnom_Penh\"\n",
    "# url =\"https://ja.wikipedia.org/wiki/%E5%B8%82%E5%B7%9DSC\"\n",
    "# url = \"https://es.wikipedia.org/wiki/Club_Atlético_Pompeya\"\n",
    "url = \"https://fr.wikipedia.org/wiki/Roannais_Foot_42\"\n",
    "url = 'https://es.wikipedia.org/wiki/Deportivo_La_Guaira_Fútbol_Club_\"B\"'\n",
    "soup = fn_fetch_soup(url)\n",
    "print(soup.title.text)\n",
    "# print (soup.prettified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_fetch_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat: 30.6253 lon: 40.6833\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import polars as pl\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "###### CONVERTER DMS > DECIMAL ###################################################\n",
    "def fn_dms2dec(degrees, minutes=0, seconds=0, direction='N'):\n",
    "    decimal = degrees + minutes / 60 + float(seconds) / 3600\n",
    "    if direction in 'SWO':  # 'O' is for 'West'\n",
    "        decimal = -decimal\n",
    "    return round(decimal, 4)\n",
    "\n",
    "#\"41º01'N 7º47'O\",  \"22°13'10.12\\\"S 49°56'21.86\\\"W\", \"22°13'10.12\\\"S 49°56'21.86\\\"W\"\n",
    "def fn_dms_str_to_decimal(dms_str):\n",
    "    # Define possible symbols for degrees, minutes, and seconds\n",
    "    degree_symbols = r\"º°d\"\n",
    "    minute_symbols = r\"'’′m\"\n",
    "    second_symbols = r\"\\\"”″s\"\n",
    "\n",
    "    # Define regex pattern to match DMS strings with and without seconds\n",
    "    dms_pattern = re.compile(\n",
    "        rf\"(\\d{{1,3}})[{degree_symbols}\\s]*\"\n",
    "        rf\"(\\d{{1,2}})[{minute_symbols}\\s]*\"\n",
    "        rf\"(\\d{{1,2}}(?:\\.\\d+)?)?[\\s{second_symbols}]*\"\n",
    "        r\"\\s*([NSEOWL])\" )\n",
    "    lat_match = dms_pattern.search(dms_str)\n",
    "    lon_match = dms_pattern.search(dms_str[lat_match.end():])\n",
    "    if not lat_match or not lon_match:\n",
    "        raise ValueError(\"Invalid DMS coordinate format\")\n",
    "\n",
    "    def convert_match_to_decimal(match):\n",
    "        deg = float(match.group(1))\n",
    "        min = float(match.group(2))\n",
    "        sec = float(match.group(3)) if match.group(3) else 0.0\n",
    "        direction = match.group(4)\n",
    "        decimal = deg + (min / 60) + (sec / 3600)\n",
    "        if direction in 'SWO':  # Assume 'L' is West by default\n",
    "            decimal = -decimal\n",
    "        return decimal\n",
    "    lat = round(convert_match_to_decimal(lat_match),4)\n",
    "    lon = round(convert_match_to_decimal(lon_match),4)\n",
    "    return lat, lon\n",
    "\n",
    "##### EXTRACTOR ##############################################################\n",
    "# 1 id coordinates\n",
    "def fn_id_coordinates(soup):\n",
    "    coordinates_span = soup.find('span', id='coordinates')\n",
    "    if coordinates_span:\n",
    "        try:\n",
    "            coordinates_str = coordinates_span.find('span', class_='geo-dec').text\n",
    "        except AttributeError:\n",
    "            coordinates_str = coordinates_span.find('span', class_='geo-dms').text\n",
    "        return coordinates_str\n",
    "    else:\n",
    "        return \"no id-coordinates\"\n",
    "    \n",
    "# 2  wgCoordinates - get DMS\n",
    "def fn_wgCoordinates(soup_str):               \n",
    "    pattern = re.compile(r'\"wgCoordinates\"\\s*:\\s*\\{[^}]*\\}', re.DOTALL)\n",
    "    wgCoordinates = pattern.search(soup_str).group()\n",
    "    wgCoordinates = wgCoordinates.replace('\\n', '')\n",
    "    return wgCoordinates\n",
    "\n",
    "# 3 strCoordinates\n",
    "def fn_str_coordinates(soup_str):\n",
    "    pattern = re.compile( r'\"coordinates\"\\s*:\\s*\\[[^\\]]+\\]',re.DOTALL)\n",
    "    coordinates = pattern.search(soup_str).group()\n",
    "    coordinates = coordinates.replace('\\n', '')\n",
    "    return coordinates\n",
    "\n",
    "# 4 geo_inline\n",
    "def fn_geo_inline(soup):\n",
    "    coordinates_span = soup.find('span', class_='geo-inline')\n",
    "    if coordinates_span:\n",
    "        try:\n",
    "            coordinates_str = coordinates_span.find('span', class_='geo-dec').text\n",
    "            return coordinates_str\n",
    "            \n",
    "        except AttributeError:\n",
    "            return \"no geo_inline\"\n",
    "\n",
    "# 5 p-class\n",
    "def fn_external_text(soup):\n",
    "    coordinates_p = soup.find('p', class_='coordinates')\n",
    "    if coordinates_p:\n",
    "        try:\n",
    "            coordinates_str = coordinates_p.find('a', class_='external text').text\n",
    "            return coordinates_str\n",
    "        except AttributeError:\n",
    "            return \"external_text not found\"\n",
    "    else:\n",
    "        return \"no external text class\"\n",
    "# # 6 api\n",
    "# def fn_api_coordinates(url):\n",
    "#     endpoint = 'page/summary'\n",
    "#     page_title = url.split(\"/\")[-1]\n",
    "#     page_title=urllib.parse.unquote(page_title)\n",
    "#     url_title = page_title.replace(\"_\",\" \")\n",
    "#     page_title = urllib.parse.quote(page_title)\n",
    "#     api_url = f'https://en.wikipedia.org/api/rest_v1/{endpoint}/{page_title}'\n",
    "#     response = requests.get(api_url)\n",
    "#     summary =  response.json()\n",
    "#     coordinates = summary.get('coordinates', \"no api\")\n",
    "#     return coordinates, lat, lon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### MAIN ###################################################\n",
    "def fn_fetch_coordinates(soup):\n",
    "    global flag_home\n",
    "    lat = None\n",
    "    lon = None\n",
    "    soup_str = str(soup)\n",
    "    #print(\"get coordinates\")\n",
    "\n",
    "    # 1 wgCoordinates in soup-string: \n",
    "    # \"wgCoordinates\":{\"lat\":56.17159722222222,\"lon\":10.16303888888889}\n",
    "    if \"wgCoordinates\" in soup_str:  \n",
    "        # 1 wgCoordinates - get DMS\n",
    "        #if flag_home == True: print(\"FOUND: wgCoordinates\")  \n",
    "        pattern = re.compile(r'wgCoordinates\"\\s*:\\s*\\{[^}]*\\}', re.DOTALL)\n",
    "        wgCoordinates = pattern.search(soup_str).group()\n",
    "        wgCoordinates = wgCoordinates.replace('\\n', '')\n",
    "        lat_pattern = re.compile(r'\"lat\":(-?\\d+\\.?\\d*)')\n",
    "        lon_pattern = re.compile(r'\"lon\":(-?\\d+\\.?\\d*)')\n",
    "        lat = lat_pattern.search(wgCoordinates)\n",
    "        lon = lon_pattern.search(wgCoordinates)\n",
    "        if lat: lat = round(float(lat.group(1)),4)\n",
    "        if lon: lon = round(float(lon.group(1)),4)\n",
    "        return lat, lon\n",
    "\n",
    "    # 3 COORDINATES in ID-Atribute\n",
    "    # Formate: 11º 33' N, 104º 55' L\n",
    "    elif soup.find(id=\"coordinates\"):\n",
    "            \n",
    "            #if flag_home == True: print(\"FOUND id-coordinates\")\n",
    "            #lat,lon = fn_coordinates_in_id(soup)\n",
    "            coordinates_div = soup.find('div', id='coordinates')\n",
    "            if coordinates_div:\n",
    "                coordinates_text = coordinates_div.find('a', class_='external text').text\n",
    "                print(\"DMS_str in id coordinates:\", coordinates_text)\n",
    "                lat, lon = fn_dms_str_to_decimal(coordinates_text)\n",
    "                return lat, lon\n",
    "\n",
    "    # 2 coordinates in soup_str\n",
    "    elif  \"coordinates\" in soup_str:\n",
    "        #if flag_home == True: print(\"FOUND: coordinates in soup_str\")\n",
    "        pattern = re.compile(r'\"coordinates\":\\[(\\-?\\d+\\.\\d+),(\\-?\\d+\\.\\d+)\\]',re.DOTALL)\n",
    "        match = pattern.search(soup_str)\n",
    "        if match:\n",
    "            lat, lon = match.groups()\n",
    "            lat = round(float(lat),4)\n",
    "            lon = round(float(lon),4)\n",
    "            return lat, lon\n",
    "\n",
    "    # 4 COORDINATES in GEO-INLINE   \n",
    "    elif soup.find('span', class_='geo-inline'):\n",
    "        if soup.find('span', class_='geo-inline'):\n",
    "            if flag_home == True: print(\"coordinates in 'geo-inline'\")\n",
    "            try:\n",
    "                # Find the span with class 'geo-inline'\n",
    "                geo_inline_tag = soup.find('span', class_='geo-inline')\n",
    "                #print(geo_inline_tag)\n",
    "                if geo_inline_tag:\n",
    "                    # Extract DMS coordinates if available\n",
    "                    lat_tag = geo_inline_tag.find('span', class_='latitude')\n",
    "                    lon_tag = geo_inline_tag.find('span', class_='longitude')\n",
    "                    if lat_tag and lon_tag:\n",
    "                        lat_text = lat_tag.get_text(strip=True)\n",
    "                        lon_text = lon_tag.get_text(strip=True)\n",
    "                        coordinates_str = f\"{lat_text} {lon_text}\"\n",
    "                        lat, lon = fn_dms_str_to_decimal(coordinates_str)\n",
    "\n",
    "                        # # parse coordinates - correct???\n",
    "                        # lat_text = lat_text.replace('°', ' ').replace('′', ' ').replace('″', ' ')\n",
    "                        # lon_text = lon_text.replace('°', ' ').replace('′', ' ').replace('″', ' ')\n",
    "                        # lat_parts = lat_text.split()\n",
    "                        # lon_parts = lon_text.split()\n",
    "\n",
    "                        # # Extract latitude and longitude parts\n",
    "                        # lat_deg = int(lat_parts[0])\n",
    "                        # lat_min = int(lat_parts[1])\n",
    "                        # lat_dir = lat_parts[2]\n",
    "                        # lon_deg = int(lon_parts[0])\n",
    "                        # lon_min = int(lon_parts[1])\n",
    "                        # lon_dir = lon_parts[2]\n",
    "\n",
    "                        # # Convert to decimal\n",
    "                        # lat = lat_deg + lat_min / 60\n",
    "                        # lon = lon_deg + lon_min / 60\n",
    "\n",
    "                        # if lat_dir == 'S':\n",
    "                                #     lat = -lat\n",
    "                                # if lon_dir == 'W':\n",
    "                                #     lon = -lon\n",
    "\n",
    "                                # # Round to 6 decimal places\n",
    "                                # lat = round(lat, 6)\n",
    "                                # lon = round(lon, 6)\n",
    "\n",
    "            except Exception as e:\n",
    "                lat = None\n",
    "                lon = None\n",
    "            return lat, lon\n",
    "    \n",
    "    # 5 p class = coordinates\n",
    "    elif soup.find('p', class_='coordinates'):\n",
    "        print(\"externalCoordinates\")\n",
    "        coordinates = fn_external_text(soup)\n",
    "        lat, lon = fn_dms_str_to_decimal(coordinates_str)\n",
    "        return coordinates, lat, lon\n",
    "    \n",
    "    flag_home = False\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "# FETCH COORDINATES ################################################################################################\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "flag_home = True\n",
    "url = 'https://tr-m-wikipedia-org.translate.goog/wiki/Akyazı?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de'\n",
    "#url =f\"https://sh-m-wikipedia-org.translate.goog/wiki/Zagreb?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de\"\n",
    "#url =\"https://pt-m-wikipedia-org.translate.goog/wiki/S%C3%A3o_Carlos_(S%C3%A3o_Paulo)?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de\"\n",
    "#url =\"https://pt.m.wikipedia.org/wiki/Est%C3%A1dio_Municipal_Prof._Lu%C3%ADs_Augusto_de_Oliveira\"\n",
    "\n",
    "#url =\"https://en.wikipedia.org/wiki/Seaside_A.F.C.\"  # no coordinates\n",
    "#url = \"https://pt-m-wikipedia-org.translate.goog/wiki/S%C3%A3o_Filipe_(Fogo)?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de\"\n",
    "#url =\"https://pt-m-wikipedia-org.translate.goog/wiki/Est%C3%A1dio_Bento_de_Abreu_Sampaio_Vidal?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de\"\n",
    "\n",
    "#url = \"https://pt-m-wikipedia-org.translate.goog/wiki/Guar%C3%A1_(Distrito_Federal)?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Amorebieta-Etxano\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Laredo,_Texas\"\n",
    "#url = \"https://pt.wikipedia.org/wiki/Phnom_Penh\"\n",
    "#url =\"https://da.wikipedia.org/wiki/Aarhus_V\"\n",
    "session = requests.Session()\n",
    "response = session.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "lat,lon = fn_fetch_coordinates(soup)\n",
    "print(\"lat:\",lat,\"lon:\",lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "town in South Lanarkshire, Scotland, UK\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def fn_headline(soup):\n",
    "    soup_str = str(soup)\n",
    "    pattern = re.compile(r'\"headline\":\"([^\"]+)\"')\n",
    "    match = pattern.search(soup_str)\n",
    "    if match:\n",
    "        headline = match.group(1)\n",
    "        headline = headline.encode('utf-8').decode('unicode_escape')\n",
    "    else:\n",
    "        headline = \"no headline\"\n",
    "    return headline\n",
    "\n",
    "url =\"https://en.wikipedia.org/wiki/East_Kilbride\"\n",
    "session = requests.Session()\n",
    "response = session.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(fn_headline(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_q_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q48808121\n",
      "('Q48808121', 'No description available')\n",
      "https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q48808121&format=json&props=descriptions\n",
      "Q Q48808121 No description available\n",
      "('Q48808121', 'No description available')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_wikidata_urls(soup):\n",
    "    soup_str = str(soup)\n",
    "    pattern = re.compile(r'https://[^\"]*\\.wikidata\\.org[^\"]*')\n",
    "    wikidata_urls = pattern.findall(soup_str)\n",
    "    pattern = re.compile(r'^Q\\d+$')  \n",
    "    for url in wikidata_urls:\n",
    "        last_part = url.split(\"/\")[-1]\n",
    "        if pattern.match(last_part):\n",
    "            q_number = last_part\n",
    "            return q_number\n",
    "\n",
    "def fn_q_number(url):\n",
    "    global flag_home\n",
    "    q_number = None    \n",
    "    response = requests.get(url)\n",
    "    html_str = response.text\n",
    "    \n",
    "    pattern = re.compile(r'https://[^\"]*\\.wikidata\\.org[^\"]*')\n",
    "    wikidata_urls = pattern.findall(html_str)\n",
    "    pattern = re.compile(r'^Q\\d+$')  \n",
    "    for url in wikidata_urls:\n",
    "        last_part = url.split(\"/\")[-1]\n",
    "        if pattern.match(last_part):\n",
    "            q_number = last_part\n",
    "            flag_home = False\n",
    "            return q_number\n",
    "\n",
    "def fn_q_api(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #print(\"RESPONSE\",response[:1000])\n",
    "    response_text = response.text\n",
    "    #print(\"RESPONSE TEXT\", response_text[:1000])\n",
    "    \n",
    "    soup_str = str(soup)\n",
    "    #print(\"SOUP STR\", soup_str[:1000])\n",
    "    pattern = r'/Q\\d+'\n",
    "    match = re.search(pattern, soup_str)\n",
    "    #print(soup_str)\n",
    "    #print(\"MATCH\",match)\n",
    "    if match:\n",
    "        q_number = match.group()[1:]\n",
    "        q_url = f'https://www.wikidata.org/w/api.php?action=wbgetentities&ids={q_number}&format=json&props=descriptions'\n",
    "        #print(q_url)\n",
    "        response = requests.get(q_url)\n",
    "        data = response.json()\n",
    "        q_description = data['entities'][q_number]['descriptions'].get('en', {}).get('value', 'No description available')\n",
    "    else:\n",
    "        q_number = None\n",
    "        q_description = None\n",
    "    return q_number, q_description\n",
    "\n",
    "\n",
    "def fn_q_api_fetcher(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    soup_str = str(soup)\n",
    "\n",
    "    pattern = re.compile(r'https://[^\"]*\\.wikidata\\.org[^\"]*')\n",
    "    wikidata_urls = pattern.findall(soup_str)\n",
    "    pattern = re.compile(r'^Q\\d+$')  \n",
    "    for url in wikidata_urls:\n",
    "        last_part = url.split(\"/\")[-1]\n",
    "        if pattern.match(last_part):\n",
    "            q_number = last_part\n",
    "# get description from wikidata\n",
    "            q_url = f'https://www.wikidata.org/w/api.php?action=wbgetentities&ids={q_number}&format=json&props=descriptions'\n",
    "            print(q_url)\n",
    "            response = requests.get(q_url)\n",
    "            data = response.json()\n",
    "            q_description = data['entities'][q_number]['descriptions'].get('en', {}).get('value', 'No description available')       \n",
    "    print(\"Q\", q_number, q_description)        \n",
    "    return  q_number, q_description\n",
    "\n",
    "\n",
    "# Example usage\n",
    "flag_home = True\n",
    "url = \"https://cs.wikipedia.org/wiki/TJ_H%C3%A1j_ve_Slezsku\"\n",
    "url = \"https://sh.wikipedia.org/wiki/NK_Nur_Zagreb\"\n",
    "#url = \"https://fr.wikipedia.org/wiki/Association_Sportive_Villers_Houlgate_Côte_Fleurie\"\n",
    "url =\"https://en.wikipedia.org/wiki/Marbella_FC\"\n",
    "url =\"https://it.wikipedia.org/wiki/Valle_d'Aosta_Calcio\"\n",
    "#soup = fn_get_soup(url)\n",
    "# response = requests.get(url)\n",
    "# #print(type(response))\n",
    "# html_str = response.text\n",
    "print(fn_q_number(url))\n",
    "print(fn_q_api(url))\n",
    "print(fn_q_api_fetcher(url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_country from coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The country is: India\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
    "\n",
    "def get_country_from_coordinates(lat, lon):\n",
    "    geolocator = Nominatim(user_agent=\"my_geopy_application\")  # Use a descriptive user agent\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True, language='en', timeout=10)\n",
    "        if location:\n",
    "            address = location.raw.get('address', {})\n",
    "            country = address.get('country', 'Unknown')\n",
    "            return country\n",
    "        else:\n",
    "            return 'not found for coords'\n",
    "    except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# lat = 49.7989\n",
    "# lon = 30.1153\n",
    "lat = 12.5962\n",
    "lon = 77.3834\n",
    "country = get_country_from_coordinates(lat, lon)\n",
    "print(f\"The country is: {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The country is: Україна\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "def get_country_from_coordinates(lat, lon):\n",
    "    url = f\"https://nominatim.openstreetmap.org/reverse?format=json&lat={lat}&lon={lon}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    country = data.get('address', {}).get('country')\n",
    "    return country\n",
    "\n",
    "lat = 49.7989\n",
    "lon = 30.1153\n",
    "# lat = 12.5962 \n",
    "# lon = 56.0466\n",
    "country = get_country_from_coordinates(lat, lon)\n",
    "print(f\"The country is: {country}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_paragraph and loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOUP TITLE: Clube Atlético Taboão da Serra - Wikipedia\n",
      "{'club_description': 'Clube Atlético Taboão da Serra, or simply Taboão da '\n",
      "                     'Serra, is a Brazilian football team based in Taboão da '\n",
      "                     'Serra, São Paulo, founded in 1985.',\n",
      " 'club_headline': 'football club',\n",
      " 'club_name': 'Clube Atlético Taboão da Serra',\n",
      " 'club_q_number': 'Q5136492',\n",
      " 'club_url': 'https://en.wikipedia.org/wiki/Clube_Atlético_Taboão_da_Serra',\n",
      " 'loc': 'Football team',\n",
      " 'loc_country': None,\n",
      " 'loc_href': 'https://en.wikipedia.org/wiki/Football_team',\n",
      " 'loc_lat': None,\n",
      " 'loc_lon': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import polars as pl\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "import pprint\n",
    "\n",
    "def fn_fetch_loc(paragraph,base_url):\n",
    "    loc = loc_href = loc_coordinates =   None\n",
    "\n",
    "    paragraph_str = str(paragraph)\n",
    "    #print(\"PARAGRAPH\", paragraph_str)\n",
    "    global soup\n",
    "\n",
    "    # \"in\" oder \"from\" zuerst?\n",
    "    if paragraph:\n",
    "        para = paragraph.text.split(\" \")\n",
    "        try:\n",
    "            index_in = para.index(\"in\")\n",
    "        except:\n",
    "            index_in = 1000\n",
    "        try:\n",
    "            index_from = para.index(\"from\")\n",
    "        except:\n",
    "            index_from = 1000\n",
    "   # print(\"in\", index_in, \"from\", index_from)\n",
    "\n",
    "# FROM\n",
    "    if index_from < index_in:\n",
    "        #print(\"fom-version\", index_in, index_from)\n",
    "        # version 1: from 20 a-tags\n",
    "        for a in soup.find_all('a', href=True)[:20]:\n",
    "            if 'from ' in a.get_text():\n",
    "                loc = a.get(\"title\")\n",
    "                loc_href = a[\"href\"]\n",
    "                break\n",
    "        # version 2: from paragraph-text\n",
    "        if \" from \" in paragraph_str:\n",
    "            from_text_parts= paragraph_str.split(\" from \",1)\n",
    "            if len(from_text_parts) > 1:\n",
    "                sib_str = from_text_parts[1]\n",
    "                new_soup=BeautifulSoup(sib_str,\"html.parser\")\n",
    "                for a_tag in new_soup.find_all(\"a\", href=True):\n",
    "                    try:\n",
    "                        loc_href = a_tag[\"href\"]\n",
    "                        if loc_href.split(\"/\")[1]==\"wiki\": loc_href= base_url + loc_href\n",
    "                        loc = a_tag[\"title\"]\n",
    "                        loc = a_tag.text\n",
    "                    except:\n",
    "                        loc_href = None\n",
    "                        loc = None\n",
    "                    break\n",
    "# IN        \n",
    "    elif index_in < index_from:\n",
    "        #print(\"in-version\")\n",
    "        # 1 check first 20 a-tags\n",
    "        for a in paragraph.find_all('a', href=True)[:20]:\n",
    "            text_lower = a.get_text().lower()\n",
    "            #print(text_lower)\n",
    "            if 'in ' in text_lower  and \"Football_in\" not in a[\"href\"] :\n",
    "                #print(\"a-tagger\", a.get_text())\n",
    "                loc = a.get(\"title\")\n",
    "                loc_href = a[\"href\"]\n",
    "                break\n",
    "        # 2 check paragraph-text\n",
    "        in_text_parts = paragraph_str.split(\" in \", 1)\n",
    "        #print(\"'in' in paragraph\", \"str-version\")\n",
    "        if \" in \" in paragraph_str:\n",
    "            if len(in_text_parts) > 1:\n",
    "                sib_str = in_text_parts[1]\n",
    "                new_soup=BeautifulSoup(sib_str,\"html.parser\")\n",
    "                for a_tag in new_soup.find_all(\"a\", href=True):\n",
    "                    try:\n",
    "                        loc_href = a_tag[\"href\"]\n",
    "                        if loc_href.split(\"/\")[1]==\"wiki\": loc_href= base_url + loc_href\n",
    "                        loc = a_tag[\"title\"]\n",
    "                        #print(\"URL loc href\",loc_href)\n",
    "                        break \n",
    "                    except:\n",
    "                        loc_href = None\n",
    "                        loc = None\n",
    "                        break\n",
    "    else:\n",
    "        print(\"Neither 'in' nor 'from' in first paragraph\")\n",
    "    \n",
    "    # remove false hrefs\n",
    "    # if loc_href is not None:\n",
    "    #     #print(\"LOC HREFFF\", loc_href, loc)\n",
    "    #     if loc_href == \"/wiki/Main_Page\":\n",
    "    #         loc_href = None\n",
    "    #     if loc_href.startswith('/w/'):\n",
    "    #         loc_href = None\n",
    "    #     if \"main page\" in loc:\n",
    "    #         loc = None\n",
    "\n",
    "    if loc_href:\n",
    "        if loc_href == \"/wiki/Main_Page\" or loc_href.startswith('/w/'):\n",
    "            loc_href = None\n",
    "\n",
    "    if loc and \"main page\" in loc.lower():\n",
    "        loc = None\n",
    "\n",
    "\n",
    "    return loc, loc_href\n",
    "\n",
    "def fn_fetch_paragraph(soup,url):\n",
    "    soup_str = str(soup)\n",
    "    # first find the paragraph\n",
    "    club_url = url\n",
    "    base_url=url.split(\"/wiki\")[0]\n",
    "    club_name = club_headline = club_q_number =description = loc_country = loc=  loc_href = loc_lat = loc_lon =  None\n",
    "\n",
    "    # TITLE\n",
    "    club_name = soup.title.string\n",
    "    club_name = club_name.split(\" - \")[0].strip()\n",
    "    club_name = club_name.split(\" – \")[0].strip()\n",
    "    \n",
    "    # HEADLINE\n",
    "    pattern = re.compile(r'\"headline\":\"([^\"]+)\"')\n",
    "    match = pattern.search(soup_str)\n",
    "    if match:\n",
    "        headline = match.group(1)\n",
    "        headline = headline.encode('utf-8').decode('unicode_escape')\n",
    "        club_headline = headline\n",
    "    \n",
    "    # Q-NUMBER\n",
    "    pattern = r'/Q\\d+'\n",
    "    match = re.search(pattern, soup_str)\n",
    "    if match:\n",
    "        club_q_number = match.group()[1:]\n",
    "\n",
    "    # PARAGRAPH\n",
    "    div = soup.find(\"div\", id = \"mw-content-text\")\n",
    "    exclude_words =[\"draft\", \"article\",\"knowledge\", \" see \"]\n",
    "    keywords = [\"football\", \"soccer\", \"team\", \"club\", \"hometown\"]\n",
    "    paragraphs = div.find_all('p')\n",
    "    for p in paragraphs:\n",
    "        text = p.get_text().strip()\n",
    "        #print(\"p text\", text)\n",
    "        text_lower = text.lower()\n",
    "        if any(exclude_word in text_lower for exclude_word in exclude_words):\n",
    "            #print(\"exclude word\")\n",
    "            continue\n",
    "\n",
    "        # DESCRIPTION, LOC, LOC_HREF\n",
    "        if any(keyword in text_lower for keyword in keywords):\n",
    "            description = text.replace(\"\\n\",\"\").replace(\" .\", \".\").replace(\" ,\", \",\")\n",
    "            #print(description)\n",
    "            loc, loc_href = fn_fetch_loc(p,base_url)\n",
    "            #print(\"LOC HREF\",loc_href)\n",
    "            if not loc_href:\n",
    "                #print(\"NO loc_href from paragraph:\",loc_href)\n",
    "                loc = \"No loc-info from paragraph\"\n",
    "\n",
    "# COORDINATES\n",
    "            if loc_href is not None:\n",
    "                #print(\"LOC HREF\",loc_href)\n",
    "                response = requests.get(loc_href)\n",
    "                soup_href = BeautifulSoup(response.content, 'html.parser')\n",
    "                loc_lat, loc_lon = fn_fetch_coordinates(soup_href)\n",
    "            else:\n",
    "                try:\n",
    "                    url = 'https://www.wikidata.org/w/api.php'\n",
    "                    params = {'action': 'wbgetentities', 'ids': club_q_number, 'format': 'json', 'props': 'claims'}\n",
    "                    data = requests.get(url, params=params).json()\n",
    "                    coords = data['entities'][club_q_number]['claims']['P625'][0]['mainsnak']['datavalue']['value']\n",
    "                    loc_lat, loc_lon = coords['latitude'], coords['longitude']\n",
    "                except:\n",
    "                    if \"Wikipedia\" not in soup.title.text:\n",
    "                        loc = \"Potential Page Translation Problem\"\n",
    "                    pass\n",
    "# COUNTRY\n",
    "            if loc_lat is not None and loc_lat != \"need\":\n",
    "                #print(\"LOC LAT\", loc_lat)\n",
    "                loc_country = get_country_from_coordinates(loc_lat, loc_lon)\n",
    "            break\n",
    "        else:\n",
    "            #if flag_home: print(\"NO KEYWORD found in paragraph\")\n",
    "            description = \"No keyword in paragraph\"\n",
    "\n",
    "    dict_page = {\n",
    "        \"club_q_number\": club_q_number,\n",
    "        \"club_url\": club_url,\n",
    "        \"club_name\": club_name,\n",
    "        \"club_headline\" : club_headline,\n",
    "        \"club_description\" : description,\n",
    "        \"loc\" : loc,\n",
    "        \"loc_href\" : loc_href,\n",
    "        \"loc_lat\":loc_lat,\n",
    "        \"loc_lon\": loc_lon,\n",
    "        \"loc_country\": loc_country}\n",
    "    return dict_page\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# url = \"https://fr.wikipedia.org/wiki/Igaci_Futebol_Clube\"\n",
    "# #url =\"https://ar.wikipedia.org/wiki/نادي_بلدية_تولال\"\n",
    "# #url =\"https://translate.google.com/translate?sl=auto&tl=en&u=https://cs.wikipedia.org/wiki/TJ_Novoměstský_Kladno\"\n",
    "# #url =\"https://cs.wikipedia.org/wiki/TJ_Novom%C4%9Bstsk%C3%BD_Kladno\"\n",
    "# url =\"https://cs.wikipedia.org/wiki/TJ_Háj_ve_Slezsku\"\n",
    "url =\"https://tr.wikipedia.org/wiki/AS_Akyazıspor\"\n",
    "#url = \"https://sh.wikipedia.org/wiki/NK_Nur_Zagreb\"\n",
    "#url =\"https://ar.wikipedia.org/wiki/نادي_النهضة_(ليبيا)\"\n",
    "#url =\"https://en.wikipedia.org/wiki/Blackfield_&_Langley_F.C.\"\n",
    "url =\"https://en.wikipedia.org/wiki/USM_Alger_Reserves_and_Academy\"\n",
    "#url = \"https://ar.wikipedia.org/wiki/نادي_بلدية_تولال\"\n",
    "#url = \"https://cs.wikipedia.org/wiki/TJ_Novoměstský_Kladno\"\n",
    "#url =\"https://ja.wikipedia.org/wiki/市川SC\"\n",
    "#url = \"https://translate.google.com/translate?sl=auto&tl=en&u=https://fr.wikipedia.org/wiki/Roannais_Foot_42\"\n",
    "#url =\"https://fr.wikipedia.org/wiki/Roannais_Foot_42\"\n",
    "#url =\"https://en.wikipedia.org/wiki/Jalal-Abad\"\n",
    "url =\"https://fr.wikipedia.org/wiki/KFC_Lille\"\n",
    "url =\"https://pt.wikipedia.org/wiki/Associação_Desportiva_e_Recreativa_de_Tarouquense\"\n",
    "url =\"https://en.wikipedia.org/wiki/Hapoel_Kiryat_Shalom_F.C.\"\n",
    "url =\"https://en.wikipedia.org/wiki/Darlaston_Town_F.C.\"\n",
    "url =\"https://es.wikipedia.org/wiki/Deportivo_La_Guaira_Fútbol_Club_\"\n",
    "url =\"https://en.wikipedia.org/wiki/Clube_Atlético_Taboão_da_Serra\"   # falsch!!!\n",
    "\n",
    "flag_home = True\n",
    "soup = fn_fetch_soup(url)\n",
    "print(\"SOUP TITLE:\",soup.title.text)\n",
    "\n",
    "dict_page = fn_fetch_paragraph(soup,url)\n",
    "pprint.pprint(dict_page)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_fetch_infobox_jerseys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red [255, 0, 0], white [255, 255, 255], None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import webcolors\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# 'Home' and 'Away' in a single tr\n",
    "def fn_jerseys(soup):\n",
    "    #global flag_home\n",
    "    jersey_home = jersey_away =jersey_third = None\n",
    "    #print(\"JERSEY SOUP ARRIVED\", soup)\n",
    "    jerseys = []\n",
    "    infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "    #if flag_home == True: print(\"JERSEY infobox found\")\n",
    "    #print(infobox.prettify())\n",
    "    #print(type(infobox))\n",
    "    #print(infobox.title.string())\n",
    "    def closest_colour(requested_colour):\n",
    "        min_colours = {}\n",
    "        for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
    "            r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "            rd = (r_c - requested_colour[0]) ** 2\n",
    "            gd = (g_c - requested_colour[1]) ** 2\n",
    "            bd = (b_c - requested_colour[2]) ** 2\n",
    "            min_colours[(rd + gd + bd)] = name\n",
    "        return min_colours[min(min_colours.keys())]\n",
    "\n",
    "    def get_colour_name(requested_colour):\n",
    "        try:\n",
    "            closest_name = actual_name = webcolors.rgb_to_name(requested_colour)\n",
    "        except ValueError:\n",
    "            closest_name = closest_colour(requested_colour)\n",
    "            actual_name = None\n",
    "        return actual_name, closest_name\n",
    "\n",
    "    if infobox: \n",
    "        nested_table = infobox.find('table')\n",
    "        #print(\"JERSEY nested_table found\",len(nested_table.find_all('td')))\n",
    "\n",
    "        if nested_table:\n",
    "            for td in nested_table.find_all('td'):\n",
    "                bg_colors = []\n",
    "                background_colors = []\n",
    "                # BG_COLORS\n",
    "                if 'bgcolor' in td.attrs:\n",
    "                    #print(\"bgcolor\")\n",
    "                    bg_colors.append(td['bgcolor'].strip())\n",
    "\n",
    "                for nested_td in td.find_all('td'):\n",
    "                    if 'bgcolor' in nested_td.attrs:\n",
    "                        bg_colors.append(nested_td['bgcolor'].strip())\n",
    "                \n",
    "                # BACKGROUND COLORS\n",
    "                divs = td.find_all('div', style=True)\n",
    "                for div in divs:\n",
    "                    styles = div['style'].split(';')\n",
    "                    for style in styles:\n",
    "                        if 'background-color' in style:\n",
    "                            #print(\"background-color found. Style:\",style)\n",
    "                            color = style.split(':')[1].strip()\n",
    "                            #print(\"COLOR:\", color)\n",
    "                            background_colors.append(color)\n",
    "                \n",
    "                # the second bgcolor or background-color in td\n",
    "                if len(bg_colors) >= 2:\n",
    "                    hex_color = bg_colors[1]\n",
    "                elif len(background_colors) >= 2:\n",
    "                    hex_color = background_colors[1]\n",
    "                    if hex_color == \"#00000\": hex_color = \"#000000\"\n",
    "                    if hex_color ==\"#FFFFFFF\": hex_color =\"#FFFFFF\"\n",
    "                    #print(\"HEX\", hex_color)\n",
    "                else:\n",
    "                    continue  # Skip if there are not enough colors\n",
    "\n",
    "                # wenn hex_color \"#\" ist\n",
    "                if hex_color == \"#\":\n",
    "                        #print(\"hexcolor is '#'\",hex_color)\n",
    "                        jerseys.append(\"#\")\n",
    "                        #pass\n",
    "                \n",
    "                # wenn hex_color echte hex-bezeichnung ist \n",
    "                elif re.match(r'^#[0-9a-fA-F]{6}$', hex_color):\n",
    "                    #print(\"hexcolors is hex-term \", hex_color)\n",
    "                    rgb_object = webcolors.hex_to_rgb(hex_color)\n",
    "                    #print(\"rgb\", rgb_object)\n",
    "                    if rgb_object:\n",
    "                        #rgb = list(map(int, re.findall(r'\\d+', str(rgb_object))))\n",
    "                        rgb = [rgb_object.red, rgb_object.green, rgb_object.blue]\n",
    "                        actual, nearest = get_colour_name(rgb)\n",
    "                        try:\n",
    "                            actual = actual\n",
    "                        except:\n",
    "                            actual = \"\"\n",
    "                        try:\n",
    "                            nearest = nearest\n",
    "                            jerseys.append(f\"{nearest} {rgb}\") \n",
    "                            #print(\"Nearest\", nearest, rgb)\n",
    "                        except:\n",
    "                            nearest = \"\"\n",
    "                        # if flag_home == True: print(f\"Existing jersey COLORS: {actual} - closest: {nearest} - RGB: {rgb} - hex: {hex_color}\")\n",
    "                \n",
    "                # wenn hex_color #colorword ist\n",
    "                elif re.match(r'^#_?[a-z]+$', hex_color):\n",
    "                    #print(\"colorword\", hex_color)\n",
    "                    jerseys.append(f\"{hex_color[1:]}\")\n",
    "                    # match = re.match(r'^_[a-z]+$', hex_color)\n",
    "                    # if match:\n",
    "                    #     #pattern_name = match.group(0)\n",
    "                    #     #print(\"hexcolor is pattern\", pattern_name)\n",
    "                    #     jerseys.append(f\"{hex_color[1:]}, \")                \n",
    "\n",
    "                else:\n",
    "                    print(hex_color, \"is no hexmatch\")  \n",
    "                    jerseys.append(\"no match\")\n",
    "                hex_color = \"\"        \n",
    "            \n",
    "            try: \n",
    "                jersey_home = jerseys[0]\n",
    "            except: \n",
    "                jersey_home = None\n",
    "            try:\n",
    "                jersey_away = jerseys[1]\n",
    "            except: \n",
    "                jersey_away = None\n",
    "            try:\n",
    "                jersey_third = jerseys[2]\n",
    "            except:\n",
    "                jersey_third = None\n",
    "    \n",
    "        #print(\"No nested_table found\")\n",
    "    #infobox = None\n",
    "    #flag_home = False\n",
    "    return jersey_home, jersey_away, jersey_third\n",
    "\n",
    "# JERSEY COLORS ##############################################################\n",
    "# url = \"https://en.wikipedia.org/wiki/ES_Collo\"\n",
    "# # url = \"https://sh.wikipedia.org/wiki/NK_Nur_Zagreb\"\n",
    "# # #url =\"https://en.wikipedia.org/wiki/Paris_Saint-Germain_F.C.\"\n",
    "# # #url = \"https://en.wikipedia.org/wiki/FC_Bayern_Munich\"\n",
    "# #url =\"https://en.wikipedia.org/wiki/Blackfield_&_Langley_F.C.\"\n",
    "# #url =\"https://nn.wikipedia.org/wiki/Valestrand_Hjellvik_Fotballklubb\"\n",
    "# url = \"https://en.wikipedia.org/wiki/Arsenal_F.C.\"\n",
    "# #url = \"https://cs.wikipedia.org/wiki/TJ_Háj_ve_Slezsku\"\n",
    "url =\"https://en.wikipedia.org/wiki/Tuwaiq_Club\"\n",
    "url =\"https://en.wikipedia.org/wiki/Union_Touring_Łódź\"\n",
    "url = \"https://en.wikipedia.org/wiki/Capital_City_F.C.\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Ashton_Town_F.C.\"\n",
    "#url =\"https://en.wikipedia.org/wiki/UD_Aretxabaleta\"\n",
    "#url =\"https://en.wikipedia.org/wiki/Borussia_Dortmund\"\n",
    "url =\"https://en.wikipedia.org/wiki/Blackfield_&_Langley_F.C.\"\n",
    "url =\"https://en.wikipedia.org/wiki/Wollongong_Wolves_FC\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#flag_home = True\n",
    "#infobox = soup.find('table', class_='infobox')\n",
    "#print(type(infobox))\n",
    "jersey_home, jersey_away, jersey_third = fn_jerseys(soup)\n",
    "print(f\"{jersey_home}, {jersey_away}, {jersey_third}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_fetch_infobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'info_away_jersey': None,\n",
      " 'info_colors': None,\n",
      " 'info_full_name': None,\n",
      " 'info_home_jersey': None,\n",
      " 'info_nickname': None,\n",
      " 'info_short_name': None,\n",
      " 'info_stadium_capacity': None,\n",
      " 'info_third_jersey': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "from dateutil import parser\n",
    "from unidecode import unidecode\n",
    "from collections import OrderedDict\n",
    "import pprint\n",
    "\n",
    "def fn_fetch_infobox(soup):\n",
    "    soup_str = str(soup)\n",
    "    infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "\n",
    "    stadium_name = stadium_href_title = stadium_href = stadium_in_name = stadium_in_href_title= stadium_in_href = full_name = founded = nickname = short_name = stadium_capacity = colors = home_jersey = away_jersey = third_jersey = None\n",
    "    capacity_names = [\"capacity\", \"seating capacity\", \"places\",\"ability\"]\n",
    "    full_names =  [\"full name\", \"name\", 'full name of the club', \"full title\", \"long name\", \"title\"]\n",
    "    no_gos = [\"page does not exist\", \"not yet drafted\", \"page not found\", \"page not available\", \"not written yet\"]\n",
    "    #soup = fn_fetch_soup(url)\n",
    "    #infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "    name_flag = False\n",
    "\n",
    "    for nr, tr in enumerate(infobox.find_all('tr')):\n",
    "        if len(tr.find_all(['td', 'th'], recursive=False)) !=2: continue\n",
    "\n",
    "        # first tag in row:\n",
    "        first_tag = tr.find(['td', 'th'], recursive=False)\n",
    "        if not first_tag: continue\n",
    "        last_td = tr.find_all('td')[-1]\n",
    "\n",
    "# CAPACITY\n",
    "        for capacity_name in capacity_names:\n",
    "            if capacity_name.lower() in tr.get_text(strip=True).lower():\n",
    "                stadium_capacity = last_td.get_text(strip=True).replace(\",\",\"\")\n",
    "                stadium_capacity = re.sub(r'\\[.*?\\]', '', stadium_capacity)\n",
    "                stadium_capacity = re.sub(r'\\(.*?\\)', '', stadium_capacity)\n",
    "                stadium_capacity = re.sub(r'[^\\d]', '',stadium_capacity)\n",
    "\n",
    "# FULL NAME\n",
    "        if name_flag == False:\n",
    "            if any(full_name.lower() in first_tag.get_text(strip=True).lower() for full_name in full_names):\n",
    "                full_name = last_td.get_text(strip=True)\n",
    "                name_flag = True\n",
    "\n",
    "# NICK NAME\n",
    "        if \"nick\" in first_tag.get_text(strip=True).lower(): \n",
    "            parts = []\n",
    "            for tag in last_td.find_all(['i','li', 'a', 'br']):\n",
    "                text = tag.get_text(strip=True)\n",
    "                if text:\n",
    "                    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "                    text = text.replace(\"citation needed\",\"\")\n",
    "                    parts.append(text)\n",
    "            nickname = ', '.join(parts)\n",
    "            nickname = re.sub(r',\\s*,', ', ', nickname).strip(', ')\n",
    "\n",
    "# SHORT NAME\n",
    "        if \"short\" in first_tag.get_text(strip=True).lower(): \n",
    "           # print(\"short\")\n",
    "            parts = []\n",
    "            #print(last_td)\n",
    "            # vom td-tag selbst\n",
    "            td_text = last_td.get_text(strip=True)\n",
    "            if td_text:\n",
    "                parts.append(re.sub(r'\\[\\d+\\]', '', td_text))\n",
    "\n",
    "            # text von anderen tags\n",
    "            for tag in last_td.find_all(['i','li', 'a', 'br']):\n",
    "        \n",
    "                text = tag.get_text(strip=True)\n",
    "               \n",
    "                if text:\n",
    "                    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "                    parts.append(text)\n",
    "            short_name = ', '.join(parts)\n",
    "            short_name = re.sub(r',\\s*,', ', ', short_name).strip(', ')\n",
    "            #print(short_name)\n",
    "\n",
    "# COLOR\n",
    "        if \"color\" in first_tag.get_text(strip=True).lower(): \n",
    "            parts = []\n",
    "            for tag in last_td.find_all(['i','li', 'a', 'br']):\n",
    "                text = tag.get_text(strip=True)\n",
    "                if text:\n",
    "                    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "                    parts.append(text)\n",
    "            colors = ', '.join(parts)\n",
    "            colors = re.sub(r',\\s*,', ', ', colors).strip(', ')\n",
    "\n",
    "# JERSEY\n",
    "        home_jersey,away_jersey,third_jersey = fn_jerseys(soup)\n",
    "\n",
    "    dict_infobox = {\n",
    "        \"info_full_name\": full_name,\n",
    "        \"info_nickname\": nickname,\n",
    "        \"info_short_name\" : short_name,\n",
    "        \"info_stadium_capacity\" : stadium_capacity,\n",
    "        \"info_colors\" : colors,\n",
    "        \"info_home_jersey\" : home_jersey,\n",
    "        \"info_away_jersey\":away_jersey,\n",
    "        \"info_third_jersey\": third_jersey\n",
    "    }\n",
    "\n",
    "    return dict_infobox\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# stadium_names = [\"stadium\", \"stadion\", \"estade\", \"name of the stadium\", \"ground\", \"home field\", \"venue\", \"football field\", \"arena\", \"home arena\", \"home track\", \"complex\", \"home court\", \"pitch\"]\n",
    "# no_gos = [\"page does not exist\", \"not yet drafted\", \"page not found\", \"page not available\", \"not written yet\"]\n",
    "url = 'https://en.wikipedia.org/wiki/Tritium_Calcio_1908' # stadium + location\n",
    "#url = \"https://en.wikipedia.org/wiki/Arsenal_F.C.\"\n",
    "#url = \"https://translate.google.com/translate?sl=auto&tl=en&u=https://sh.wikipedia.org/wiki/NK_Nur_Zagreb\"\n",
    "#url =\"https://en.wikipedia.org/wiki/Aurich–Cañaña\"\n",
    "#url = \"https://en.wikipedia.org/wiki/BVB_International_Academy_Waterloo\"\n",
    "#url =\"https://en.wikipedia.org/wiki/Okoume_FC\"\n",
    "#url = \"https://en.wikipedia.org/wiki/Racing_Club_de_France_Football\" # 2 stadiums\n",
    "#url =\"https://translate.google.com/translate?sl=auto&tl=en&u=https://es.wikipedia.org/wiki/Club_Atlético_Pompeya\"\n",
    "#url = \"https://es.wikipedia.org/wiki/Club_Atlético_Pompeya\"\n",
    "#url =\"https://pt.wikipedia.org/wiki/Sociedade_Esportiva_Sidrolândia\"\n",
    "#url =\"https://en.wikipedia.org/wiki/Aurich–Cañaña\"\n",
    "#url = \"https://cs.wikipedia.org/wiki/Meteor_Louny\"\n",
    "#url = \"https://en.wikipedia.org/wiki/FC_Bayern_Munich\"\n",
    "url =\"https://en.wikipedia.org/wiki/Borussia_Dortmund\"\n",
    "#url =\"https://en.wikipedia.org/wiki/CF_Univer_Comrat\" # stadion br ort, land\n",
    "#url =\"https://nn.wikipedia.org/wiki/Valestrand_Hjellvik_Fotballklubb\"\n",
    "#url =\"https://sh.wikipedia.org/wiki/NK_Nur_Zagreb\"\n",
    "url =\"https://en.wikipedia.org/wiki/Darlaston_Town_F.C.\"\n",
    "url =\"https://es.wikipedia.org/wiki/Deportivo_La_Guaira_Fútbol_Club_\"\n",
    "\n",
    "\n",
    "base_url = url.split(\"/wiki\")[0]\n",
    "soup = fn_fetch_soup(url)\n",
    "soup_str = str(soup)\n",
    "infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "\n",
    "\n",
    "dict_infobox = fn_fetch_infobox(infobox)\n",
    "\n",
    "# print(f\"NAMES: {infobox_dict[\"full_name\"]}, NICKNAME: {infobox_dict[\"nickname\"]}, SHORT_NAME: {infobox_dict[\"short_name\"]}\") \n",
    "# print(f\"STADIUM Capacity: {infobox_dict[\"stadium_capacity\"]}\")\n",
    "# print(F\"JERSEY: {infobox_dict [\"home_jersey\"]}, {infobox_dict [\"away_jersey\"]}, {infobox_dict [\"third_jersey\"]}\")\n",
    "pprint.pprint(dict_infobox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_fetch_infobox_stadium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD Q Stadio di Aosta  \n",
      "a_text: Mario Puchoz Stadium a_href: https://it-m-wikipedia-org.translate.goog/wiki/Stadio_Mario_Puchoz?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de a_title: Mario Puchoz Stadium ,country: Italy ,lat 45.7372 ,lon 7.3272\n",
      "\n",
      "\n",
      "{'stadium_a_href': 'https://it-m-wikipedia-org.translate.goog/wiki/Stadio_Mario_Puchoz?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=de',\n",
      " 'stadium_a_tag_count': 1,\n",
      " 'stadium_a_tags_matching': 1,\n",
      " 'stadium_a_text': 'Mario Puchoz Stadium',\n",
      " 'stadium_a_title': 'Mario Puchoz Stadium',\n",
      " 'stadium_br_tag_count': 1,\n",
      " 'stadium_country': 'Italy',\n",
      " 'stadium_headline': 'Stadio di Aosta',\n",
      " 'stadium_lat': 45.7372,\n",
      " 'stadium_lon': 7.3272,\n",
      " 'stadium_q_description': '',\n",
      " 'stadium_q_number': '',\n",
      " 'stadium_text': 'Mario Puchoz Stadium, (2 500 seats)'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import polars as pl\n",
    "from urllib.parse import unquote\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "\n",
    "\n",
    "a_tags_matching = 0\n",
    "# stadium_name = stadium_href_title = stadium_href = stadium_in = in_href_title = in_a_tag_href = None\n",
    "\n",
    "# a_tag_count=br_tag_count=a_tags_matching=stadium_text=stadium_a_text=stadium_a_title=stadium_a_href=headline=q_number=q_description=stadium_lat=stadium_lon=stadium_country=None\n",
    "\n",
    "stadium_names = [\"stadium\", \"stadion\", \"estade\", \"name of the stadium\", \"ground\", \"home field\", \"venue\", \"football field\", \"arena\", \"home arena\", \"home track\", \"complex\", \"home court\", \"pitch\"]\n",
    "\n",
    "no_gos = [\"page does not exist\", \"not yet drafted\", \"page not found\", \"page not available\", \"not written yet\"]\n",
    "\n",
    "def fn_headline_and_q(stadium_url):\n",
    "    headline = q_number = q_description = \"\"\n",
    "    response = requests.get(stadium_url)\n",
    "    stadium_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    stadium_soup_str = str(stadium_soup)\n",
    "\n",
    "    # 1 HEADLINE\n",
    "    pattern = re.compile(r'\"headline\":\"([^\"]+)\"')\n",
    "    match = pattern.search(stadium_soup_str)\n",
    "    if match:\n",
    "        headline = match.group(1)\n",
    "        headline = headline.encode('utf-8').decode('unicode_escape')\n",
    "    else:\n",
    "        headline = \"no headline\"\n",
    "    #print(\"HEADLINE\", headline)\n",
    "   \n",
    "    # 2 Q-NUMBER and DESCRIPTION    \n",
    "    pattern = re.compile(r'https://[^\"]*\\.wikidata\\.org[^\"]*')\n",
    "    wikidata_urls = pattern.findall(stadium_soup_str)\n",
    "    pattern = re.compile(r'^Q\\d+$')  \n",
    "    for url in wikidata_urls:\n",
    "        last_part = url.split(\"/\")[-1]\n",
    "        if pattern.match(last_part):\n",
    "            q_number = last_part\n",
    "    # get description from wikidata\n",
    "            q_url = f'https://www.wikidata.org/w/api.php?action=wbgetentities&ids={q_number}&format=json&props=descriptions'\n",
    "            response = requests.get(q_url)\n",
    "            data = response.json()\n",
    "            q_description = data['entities'][q_number]['descriptions'].get('en', {}).get('value', 'No description available')        \n",
    "            break \n",
    "    print(\"HEAD Q\",headline, q_number, q_description)       \n",
    "    return headline, q_number, q_description\n",
    "\n",
    "def count_matching_a_tags(td_tag):\n",
    "    count = 0\n",
    "    for a_tag in td_tag.find_all('a'):\n",
    "        text = a_tag.get_text(strip=True)\n",
    "        title = a_tag.get('title')\n",
    "        # Check if the title attribute exists and if the title matches the text\n",
    "        if title and text == title:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def fn_fetch_stadium(soup, base_url):\n",
    "    soup_str = str(soup)\n",
    "    infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "\n",
    "    \n",
    "    stadium_list =[]\n",
    "\n",
    "    dict_stadium = {key: None for key in [\"stadium_a_tag_count\", \"stadium_br_tag_count\", \"stadium_a_tags_matching\", \"stadium_lat\", \"stadium_lon\", \"stadium_country\"]}\n",
    "    \n",
    "    dict_stadium.update({\n",
    "        \"stadium_a_text\": \"\",\n",
    "        \"stadium_a_title\": \"\",\n",
    "        \"stadium_a_href\": \"\",\n",
    "        \"stadium_headline\": \"\",\n",
    "        \"stadium_q_number\": \"\",\n",
    "        \"stadium_q_description\": \"\" })\n",
    "    \n",
    "    a_infos = []\n",
    "    url_trans = None\n",
    "    stadium_flag = False\n",
    "    coordinates_flag = False\n",
    "\n",
    "    for tr in infobox.find_all('tr'):\n",
    "        #print(nr)\n",
    "        if len(tr.find_all(['td', 'th'], recursive=False)) !=2: continue\n",
    "\n",
    "        # first tag in row:\n",
    "        first_tag = tr.find(['td', 'th'], recursive=False)\n",
    "        if not first_tag: continue\n",
    "        last_td = tr.find_all('td')[-1]\n",
    "\n",
    "        if any(stadium_name.lower() in tr.get_text(strip=True).lower() for stadium_name in stadium_names):\n",
    "            stadium_flag = True\n",
    "\n",
    "            # basic about a-tag\n",
    "            stadium_a_tag_count = len(last_td.find_all('a'))\n",
    "            stadium_a_tags_matching = count_matching_a_tags(last_td)\n",
    "            stadium_br_tag_count = len(last_td.find_all('br'))\n",
    "            dict_stadium[\"stadium_a_tag_count\"] = stadium_a_tag_count\n",
    "            dict_stadium[\"stadium_br_tag_count\"] = stadium_br_tag_count\n",
    "            dict_stadium[\"stadium_a_tags_matching\"] = stadium_a_tags_matching            \n",
    "            \n",
    "            # url_trans auf None\n",
    "            if url_trans: url_trans = None\n",
    "            \n",
    "            # td_text\n",
    "            td_text = str(last_td)\n",
    "            td_text = re.sub(r'<.*?>', '  ', td_text).strip() # remove html-tags\n",
    "            td_text = td_text.replace(\"  \",\",\") # separate by \", \"\n",
    "            td_text = re.sub(r',\\s*\\d+\\s*,', ',', td_text) # remove number between commas\n",
    "            td_text = re.sub(r'\\s*,\\s*,\\s*', ', ', td_text)  # remove multiple commas\n",
    "            td_text = re.sub(r'\\s*,+\\s*', ', ', td_text)  # remove multiple commas\n",
    "            td_text = re.sub(r' \\s*,+', '', td_text)  # remove multiple commas\n",
    "            td_text = re.sub(r'\\s*,+\\s*', ', ',td_text) # remove comma/whitespace\n",
    "            td_text = re.sub(r'\\[.*?\\]', '', td_text) # remove square brackets \n",
    "            stadium_list.append(td_text)\n",
    "                        \n",
    "            if stadium_a_tag_count > 0:\n",
    "                headline = q_number = q_description = \"\"\n",
    "                lat = lon = country = \"\"\n",
    "\n",
    "                # a-tags in a_infos\n",
    "                for element in last_td.find_all('a'):\n",
    "                    a_text = element.get_text(strip=True)\n",
    "                    if a_text == \"\": a_text =\"no text\"\n",
    "                    if \"translate\" in element[\"href\"] or \"http\" in element[\"href\"]:\n",
    "                        a_href = element['href']\n",
    "                    else:    \n",
    "                        a_href = base_url + element['href']\n",
    "                    a_title = element.get('title', '')\n",
    "                    if a_title ==\"\":a_title =\"not title\"\n",
    "                    \n",
    "                    if \"index\" in a_href or \"File:\" in a_href or \"geohack\" in a_href or any(no_go.lower() in a_title.lower() for no_go in no_gos):\n",
    "                        #print(\"index-case\")\n",
    "                        a_title = \"no title\"\n",
    "                        a_href = \"no url\"\n",
    "                    elif \"translate\" in a_href or \"wikipedia\" in a_href:\n",
    "# HEADLINE, Q-NUMBER\n",
    "                        headline,q_number, q_description = fn_headline_and_q(a_href)\n",
    "# COORDINATES \n",
    "                        #print(\"FLAG\", coordinates_flag)\n",
    "                        if coordinates_flag == False:\n",
    "                            response = requests.get(a_href)\n",
    "                            a_tag_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                            lat, lon = fn_fetch_coordinates(a_tag_soup) \n",
    "# COUNTRY                            \n",
    "                            if lat: \n",
    "                                coordinates_flag = True\n",
    "                                country = get_country_from_coordinates(lat, lon)\n",
    "                            else:\n",
    "                                country =\"unknown\" \n",
    "                        else:\n",
    "                            lat = lon  = country = \"\" \n",
    "\n",
    "                    print(\"a_text:\", a_text, \"a_href:\",a_href,\"a_title:\",a_title,\",country:\", country,\",lat\", lat, \",lon\",lon)\n",
    "                    \n",
    "                    if dict_stadium[\"stadium_a_text\"]: dict_stadium[\"stadium_a_text\"] += \", \"\n",
    "                    dict_stadium[\"stadium_a_text\"] += a_text\n",
    "\n",
    "                    if dict_stadium[\"stadium_a_title\"]: dict_stadium[\"stadium_a_title\"]+=\", \"\n",
    "                    dict_stadium[\"stadium_a_title\"]+= a_title\n",
    "\n",
    "                    if dict_stadium[\"stadium_a_href\"]: dict_stadium[\"stadium_a_href\"] += \", \"\n",
    "                    dict_stadium[\"stadium_a_href\"]+= unquote(a_href)\n",
    "\n",
    "                    if dict_stadium[\"stadium_headline\"]: dict_stadium[\"stadium_headline\"] += \", \"\n",
    "                    dict_stadium[\"stadium_headline\"]+=headline\n",
    "\n",
    "                    if dict_stadium[\"stadium_q_number\"]: dict_stadium[\"stadium_q_number\"] += \", \"\n",
    "                    dict_stadium[\"stadium_q_number\"]+=q_number \n",
    "\n",
    "                    if dict_stadium[\"stadium_q_description\"]: dict_stadium[\"stadium_q_description\"] += \", \"\n",
    "                    dict_stadium[\"stadium_q_description\"]+=q_description  \n",
    "\n",
    "                    dict_stadium[\"stadium_lat\"] = lat  \n",
    "                    dict_stadium[\"stadium_lon\"] = lon \n",
    "                    dict_stadium[\"stadium_country\"] = country\n",
    "             \n",
    "            a_tags_matching = 0\n",
    "    #print(stadium_list)\n",
    "    dict_stadium[\"stadium_text\"] = ', '.join(stadium_list)\n",
    "    if stadium_flag == False:\n",
    "        #print(\"\\n\")\n",
    "        #print(nr, url, \"no stadium\")\n",
    "        #dict_stadium[\"stadium_text\"] = \"no stadium\"\n",
    "        dict_stadium[\"notice\"]=\"no stadium\"\n",
    "        \n",
    "    return dict_stadium\n",
    "\n",
    "url = \"https://cs.wikipedia.org/wiki/TJ_H%C3%A1j_ve_Slezsku\"\n",
    "url = \"https://sh.wikipedia.org/wiki/NK_Nur_Zagreb\"\n",
    "#url =\"https://en.wikipedia.org/wiki/Blackfield_&_Langley_F.C.\"\n",
    "#url =\"https://en.wikipedia.org/wiki/ES_Collo\"\n",
    "url = \"https://en.wikipedia.org/wiki/Unami_CP\"\n",
    "url =\"https://en.wikipedia.org/wiki/Darlaston_Town_F.C.\"\n",
    "url =\"https://es.wikipedia.org/wiki/Deportivo_La_Guaira_Fútbol_Club_\"\n",
    "url =\"https://en.wikipedia.org/wiki/FC_Bayern_Munich\"\n",
    "url =\"https://fr.wikipedia.org/wiki/Royal_Stade_nivellois\"\n",
    "url =\"https://en.wikipedia.org/wiki/Hoyerswerdaer_FC\"\n",
    "url =\"https://en.wikipedia.org/wiki/Marbella_FC\"\n",
    "url = \"https://en.wikipedia.org/wiki/Persap_Alor_Pantar\"\n",
    "url =\"https://it.wikipedia.org/wiki/Valle_d'Aosta_Calcio\"\n",
    "\n",
    "base_url = url.split(\"/wiki\")[0]\n",
    "soup = fn_fetch_soup(url)\n",
    "# soup_str = str(soup)\n",
    "# infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "\n",
    "dict_stadium = fn_fetch_stadium(soup, base_url)\n",
    "\n",
    "list = []\n",
    "for key in dict_stadium.keys(): list.append(key)\n",
    "#print(list)\n",
    "\n",
    "print(\"\\n\")\n",
    "pprint.pprint(dict_stadium)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fn_fetch_infobox_founded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900-02-27\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import polars as pl\n",
    "from urllib.parse import unquote\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to convert date\n",
    "def convert_date(date_str):\n",
    "    pattern1 = r'(\\d{1,2}) (\\w+) (\\d{4})'  # For \"29 June 1945\"\n",
    "    pattern2 = r'(\\w+) (\\d{1,2})\\s*,\\s*(\\d{4})'  # For \"December 10, 2021\"\n",
    "    pattern3 = r'(\\d{2})/(\\d{2})/(\\d{4})'  # For \"24/04/1956\"\n",
    "    match1 = re.match(pattern1, date_str)\n",
    "    match2 = re.match(pattern2, date_str)\n",
    "    match3 = re.match(pattern3, date_str)\n",
    "    \n",
    "    if match1:\n",
    "        day, month, year = match1.groups()\n",
    "        date_obj = datetime.strptime(f\"{day} {month} {year}\", \"%d %B %Y\")\n",
    "        return date_obj.strftime(\"%Y-%m-%d\")\n",
    "    elif match2:\n",
    "        month, day, year = match2.groups()\n",
    "        date_obj = datetime.strptime(f\"{month} {day} {year}\", \"%B %d %Y\")\n",
    "        return date_obj.strftime(\"%Y-%m-%d\")\n",
    "    elif match3:\n",
    "        day, month, year = match3.groups()\n",
    "        date_obj = datetime.strptime(f\"{day}/{month}/{year}\", \"%d/%m/%Y\")\n",
    "        return date_obj.strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        return \"founding date problem\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# FOUNDED\n",
    "founded_terms = [\n",
    "    \"Founded\", \"Foundation\", \"Established\", \"Formation\", \"Inception\", \"Created\", \n",
    "     \"Began\", \"Started\", \"Commenced\", \"Institution\", \"Constituted\", \n",
    "    \"Incorporated\", \"Initiated\", \"Set up\", \"Launched\", \"Birth\", \"Founded as\", \n",
    "    \"Began as\", \"Created as\", \"Founded in\", \"Started in\", \"Established in\", \n",
    "    \"Organized in\", \"Formed in\", \"Instituted in\", \"Constituted in\", \n",
    "    \"Incorporated in\", \"Initiated in\", \"Originated in\", \"Established on\", \n",
    "    \"Founded on\", \"Organized on\", \"Formed on\", \"Instituted on\", \"Constituted on\", \n",
    "    \"Incorporated on\", \"Initiated on\", \"Originated on\", \"Founding date\", \n",
    "    \"Date of establishment\", \"Date of founding\", \"Date of creation\", \n",
    "    \"Date of inception\", \"Date of commencement\", \"Date of origin\", \n",
    "    \"Date started\", \"Year founded\", \"Year established\", \"year of establishment\", \"Year of foundation\", \n",
    "    \"Year of creation\", \"Year of inception\", \"Year of commencement\", \n",
    "    \"Year of origin\", \"Year started\", \"Originated from\", \"Created from\", \n",
    "    \"Started from\", \"Established from\", \"Founded from\", \"Incepted from\", \n",
    "    \"Launched from\", \"Date of launch\", \"Launch date\", \"Year of launch\", \n",
    "    \"Originally founded\", \"Originally established\", \"Originally created\", \n",
    "    \"Originally started\", \"Initially founded\", \"Initially established\", \n",
    "    \"Initially created\", \"Initially started\", \"Initially commenced\", \n",
    "    \"Initially organized\", \"Originally organized\", \"Formation date\", \n",
    "    \"Year of formation\", \"Formation year\", \"Commencement date\", \n",
    "    \"Year of commencement\", \"Year of institution\", \"Institutional date\", \n",
    "    \"Year of constitution\", \"Constitutional date\", \"Year of incorporation\", \n",
    "    \"Incorporation date\", \"Year of initiation\", \"Initiation date\", \n",
    "    \"Year of origin\", \"Original date\", \"Origin year\", \"Establishment year\", \n",
    "    \"Establishment date\", \"Historical founding\", \"Historical establishment\", \n",
    "    \"Historical creation\", \"Historical inception\", \"Historical commencement\", \n",
    "    \"Year of foundation\", \"Founding\", \"Founded in\", \"Establishment\", \n",
    "    \"Establish\", \"Set up\", \"Creation\", \"Creation date\", \"Stand\", \"Formed\", \n",
    "    \"Beginning\", \"Based\", \"Fundamentally\"\n",
    "]\n",
    "\n",
    "\n",
    "td_text = None\n",
    "founded = None\n",
    "\n",
    "def fn_founded(soup):\n",
    "    soup_str = str(soup)\n",
    "    infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "\n",
    "    founded = None\n",
    "    for tr in infobox.find_all('tr'):\n",
    "        if len(tr.find_all(['td', 'th'], recursive=False)) !=2: continue\n",
    "        first_tag = tr.find(['td', 'th'], recursive=False)\n",
    "        if not first_tag: continue\n",
    "        last_td = tr.find_all('td')[-1]\n",
    "        if any(founded_term.lower() in first_tag.get_text(strip=True).lower() for founded_term in founded_terms):\n",
    "            \n",
    "            # td_text\n",
    "            td_text = last_td.get_text(separator=\" \").strip()\n",
    "            \n",
    "            td_text = last_td.get_text(separator=' ').strip().replace('\\n', '')\n",
    "            td_text = re.sub(r'\\s+', ' ', td_text) # remove dirty commas\n",
    "            td_text = re.sub(r'\\[.*?\\]', '', td_text) # remove square brackets\n",
    "            td_text = re.sub(r'\\[.*?years.*?\\]', '', td_text) # remove (... years ago)\n",
    "            td_text = re.sub(r'\\(.*?years.*?\\)', '', td_text) # remove (..years)\n",
    "            \n",
    "            td_text = re.sub(r'\\([^()]*\\bas\\b[^()]*\\)', \"\", td_text) # remove (as ...)\n",
    "            td_text = convert_date(td_text) # convert dates\n",
    "            #print(td_text)\n",
    "            founded = td_text.split(\";\")[0] # remove part after \";\"\n",
    "            # years = re.findall(r'\\b\\d{4}\\b', td_text)\n",
    "            # founded = ', '.join(years)\n",
    "            break\n",
    "    return founded\n",
    "    \n",
    "url = \"https://cs.wikipedia.org/wiki/TJ_H%C3%A1j_ve_Slezsku\"\n",
    "url =\"https://en.wikipedia.org/wiki/ASFAG\"\n",
    "#url = \"https://sh.wikipedia.org/wiki/NK_Nur_Zagreb\"\n",
    "url =\"https://en.wikipedia.org/wiki/FC_Bayern_Munich\"\n",
    "base_url = url.split(\"/wiki\")[0]\n",
    "soup = fn_fetch_soup(url)\n",
    "soup_str = str(soup)\n",
    "infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "\n",
    "print(fn_founded(soup))\n",
    "# print(nr, url, )\n",
    "# print(nr, \"TD TEXT founded:\", td_text)\n",
    "# td_text = None\n",
    "\n",
    "#169 https://ja.wikipedia.org/wiki/VONDS市原Vert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799 https://en.wikipedia.org/wiki/Wollongong_Wolves_FC\n",
      "HEAD Q multi-sports stadium Q8030374 multi-sports stadium\n",
      "a_text: WIN Stadium a_href: https://en.wikipedia.org/wiki/WIN_Stadium a_title: WIN Stadium ,country: Australia ,lat -34.4278 ,lon 150.9025\n",
      "801 https://en.wikipedia.org/wiki/Football_Club_Afon_Novy_Afon\n",
      "HEAD Q town in Georgia Q42194 town in Georgia\n",
      "a_text: Novy Afon a_href: https://en.wikipedia.org/wiki/Novy_Afon a_title: Novy Afon ,country: Abkhazia ,lat 43.0843 ,lon 40.8174\n",
      "HEAD Q country in the Caucasus Q230 country in the Caucasus\n",
      "a_text: Georgia a_href: https://en.wikipedia.org/wiki/Georgia_(country) a_title: Georgia (country) ,country:  ,lat  ,lon \n",
      "802 https://en.wikipedia.org/wiki/Lao_Police_Club\n",
      "HEAD Q multi-use stadium in Vientiane, Laos Q1382956 multi-use stadium in Vientiane, Laos\n",
      "a_text: New Laos National Stadium a_href: https://en.wikipedia.org/wiki/New_Laos_National_Stadium a_title: New Laos National Stadium ,country: Laos ,lat 18.0619 ,lon 102.7039\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import polars as pl\n",
    "import xlwings as xw\n",
    "from urllib.parse import unquote\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import winsound\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# SAVE\n",
    "def save_to_csv(df):\n",
    "    if len(df)>0:\n",
    "        clubs = \"wikipedia_clubs.csv\"\n",
    "        if os.path.exists(clubs):\n",
    "            with open(clubs, newline='', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                headers = reader.fieldnames\n",
    "                dtype_dict = {col: pl.Utf8 for col in headers}\n",
    "                df_wikipedia_clubs = pl.read_csv(clubs,dtypes = dtype_dict)\n",
    "                \n",
    "                df_wikipedia_clubs = df_wikipedia_clubs.with_columns([pl.col(column).cast(pl.Utf8) for column in df_wikipedia_clubs.columns])\n",
    "                \n",
    "                #print(\"LEN df_wikipedia_clubs\", len(df_wikipedia_clubs.columns))\n",
    "                #print(\"LEN df\", len(df.columns))\n",
    "                df_wikipedia_clubs = df_wikipedia_clubs.vstack(df)\n",
    "                #print(\"ROWS IN DF CLUBS\",len(df_wikipedia_clubs))\n",
    "                df_wikipedia_clubs = df_wikipedia_clubs.unique(subset=[\"club_q_number\"], maintain_order=True)\n",
    "                df_wikipedia_clubs = df_wikipedia_clubs.with_columns(pl.col(\"nr\").cast(pl.Int32)).sort(\"nr\")\n",
    "                #df_wikipedia_clubs = df_wikipedia_clubs.sort(\"nr\")\n",
    "                #print(\"LEN df_wikipedia_clubs\", len(df_wikipedia_clubs))\n",
    "                df_wikipedia_clubs.write_csv(clubs) \n",
    "        else:\n",
    "            df.write_csv(clubs)\n",
    "\n",
    "# DONE urls\n",
    "done_urls = []\n",
    "clubs = \"wikipedia_clubs.csv\"\n",
    "# if os.path.exists(clubs):\n",
    "#     df_done_urls = pl.read_csv(clubs)\n",
    "#     for q in df_done_urls[\"club_url\"]: done_urls.append(q)\n",
    "if os.path.exists(clubs):\n",
    "        with open(clubs, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            headers = reader.fieldnames\n",
    "        dtype_dict = {col: pl.Utf8 for col in headers}\n",
    "        df_done_urls = pl.read_csv(clubs, dtypes=dtype_dict)\n",
    "        done_urls = df_done_urls[\"club_url\"].to_list()\n",
    "\n",
    "# MAKE URLS\n",
    "urls = []\n",
    "df_urls = pl.read_csv(\"wd_clubs.csv\")\n",
    "for url in df_urls[\"url\"]: urls.append(url)\n",
    "\n",
    "# DICTIONARY TEMPLATE\n",
    "keys = ['nr', 'club_q_number', 'club_url', 'club_name', 'club_headline', 'loc_country', 'loc', 'loc_lat', 'loc_lon', 'stadium_q_number', 'stadium_text', 'stadium_q_description', 'stadium_a_text', 'stadium_a_title', 'stadium_country', 'stadium_headline', 'stadium_lat', 'stadium_lon', 'info_stadium_capacity', 'info_colors', 'info_home_jersey', 'info_away_jersey', 'info_third_jersey', 'founded', 'info_full_name', 'info_nickname', 'info_short_name', 'club_description', 'loc_href', 'stadium_a_href', 'stadium_a_tag_count', 'stadium_br_tag_count', 'stadium_a_tags_matching','notice']\n",
    "dict_page_template = {key: None for key in keys}\n",
    "\n",
    "df = pl.DataFrame()\n",
    "frow = 690\n",
    "lrow = 2000\n",
    "dict_club = defaultdict(str)\n",
    "for nr in range(frow, lrow):\n",
    "    try:\n",
    "        url = urls[nr].strip()\n",
    "        if url in done_urls: continue\n",
    "        print(nr, url)\n",
    "        base_url = url.split(\"/wiki\")[0]\n",
    "        dict_page = dict_page_template.copy()\n",
    "        soup = fn_fetch_soup(url)\n",
    "        if soup.title.text ==\"Google Translate\":\n",
    "            dict_page[\"nr\"]=nr\n",
    "            dict_page[\"club_url\"]=url\n",
    "            dict_page[\"notice\"] = \"Google could not translate\"\n",
    "            continue\n",
    "        soup_str = str(soup)\n",
    "\n",
    "        infobox = soup.find(lambda tag: (tag.name == \"table\" or tag.name == \"div\") and tag.get(\"class\") and (\"infobox\" in tag.get(\"class\") or \"toccolours\" in tag.get(\"class\")))\n",
    "\n",
    "        if infobox:\n",
    "            # STADIUM\n",
    "            dict_stadium = fn_fetch_stadium(soup, base_url)\n",
    "            # FOUNDED\n",
    "            founded = fn_founded(soup)\n",
    "            # INFOBOX\n",
    "            dict_infobox = fn_fetch_infobox(soup)\n",
    "        else:\n",
    "            #dict_page[\"stadium_text\"] = \"no infobox\"\n",
    "            dict_page[\"notice\"] =\"no infobox\"\n",
    "            print(nr,url, \"no infobox\")\n",
    "            dict_infobox ={}\n",
    "            dict_stadium ={}\n",
    "            founded =\"\"\n",
    "\n",
    "        # PAGE:\n",
    "        dict_paragraph = fn_fetch_paragraph(soup,url)\n",
    "       # if dict_paragraph:\n",
    "        dict_page.update(dict_paragraph)\n",
    "        dict_page.update(dict_stadium)\n",
    "        dict_page.update(dict_infobox)\n",
    "        dict_page[\"founded\"] = founded\n",
    "        dict_page[\"nr\"] = nr\n",
    "\n",
    "        # DATAFRAME\n",
    "        df_page = pl.DataFrame(dict_page)\n",
    "        #print(\"LEN\", len(df_page.columns))\n",
    "        #print(nr,dict_page[\"url\"])\n",
    "        #display(df_page)\n",
    "        df_page = df_page.with_columns([pl.col(column).cast(pl.Utf8) for column in df_page.columns])\n",
    "        # print(df.columns)\n",
    "        # print(df_page.colums)\n",
    "        #print(len(df),len(df_page))\n",
    "        \n",
    "        cols = ['nr','club_q_number', 'club_url', 'club_name', 'club_headline',  'loc_country','loc', 'loc_lat', 'loc_lon',   'stadium_q_number','stadium_text','stadium_q_description', 'stadium_a_text', 'stadium_a_title', 'stadium_country', 'stadium_headline', 'stadium_lat', 'stadium_lon', 'info_stadium_capacity', 'info_colors', 'info_home_jersey', 'info_away_jersey', 'info_third_jersey', 'founded', 'info_full_name', 'info_nickname', 'info_short_name', 'club_description', 'loc_href', 'stadium_a_href','stadium_a_tag_count', 'stadium_br_tag_count', 'stadium_a_tags_matching','notice']\n",
    "        df_page = df_page.select(cols)\n",
    "        df = pl.concat([df, df_page], how = \"vertical\")\n",
    "\n",
    "        # save to csv\n",
    "        if int(nr) % 10 == 0:\n",
    "            save_to_csv(df)\n",
    "    except Exception as e:\n",
    "        winsound.Beep(1000, 500) \n",
    "        sys.exit(1) \n",
    "\n",
    "if len(df)==0:\n",
    "    print(\"no new data\")\n",
    "else:\n",
    "    save_to_csv(df)\n",
    "\n",
    "    pdf = df.to_pandas()\n",
    "    xw.view(pdf)\n",
    "\n",
    "    #print(df.columns)\n",
    "    cols = ['nr','club_q_number', 'club_url', 'club_name', 'club_headline',  'loc_country','loc', 'loc_lat', 'loc_lon',   'stadium_q_number','stadium_text','stadium_q_description', 'stadium_headline', 'stadium_a_text', 'stadium_a_title', 'stadium_country', 'stadium_lat', 'stadium_lon', 'info_stadium_capacity', 'info_colors', 'info_home_jersey', 'info_away_jersey', 'info_third_jersey', 'founded', 'info_full_name', 'info_nickname', 'info_short_name', 'club_description', 'loc_href', 'stadium_a_href','stadium_a_tag_count', 'stadium_br_tag_count', 'stadium_a_tags_matching','notice']\n",
    "    df = df.select(cols)\n",
    "    df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "34\n",
      "['nr', 'club_q_number', 'club_url', 'club_name', 'club_headline', 'loc_country', 'loc', 'loc_lat', 'loc_lon', 'stadium_q_number', 'stadium_text', 'stadium_q_description', 'stadium_a_text', 'stadium_a_title', 'stadium_country', 'stadium_headline', 'stadium_lat', 'stadium_lon', 'info_stadium_capacity', 'info_colors', 'info_home_jersey', 'info_away_jersey', 'info_third_jersey', 'founded', 'info_full_name', 'info_nickname', 'info_short_name', 'club_description', 'loc_href', 'stadium_a_href', 'stadium_a_tag_count', 'stadium_br_tag_count', 'stadium_a_tags_matching', 'notice']\n",
      "['nr', 'club_q_number', 'club_url', 'club_name', 'club_headline', 'loc_country', 'loc', 'loc_lat', 'loc_lon', 'stadium_q_number', 'stadium_text', 'stadium_q_description', 'stadium_a_text', 'stadium_a_title', 'stadium_country', 'stadium_headline', 'stadium_lat', 'stadium_lon', 'info_stadium_capacity', 'info_colors', 'info_home_jersey', 'info_away_jersey', 'info_third_jersey', 'founded', 'info_full_name', 'info_nickname', 'info_short_name', 'club_description', 'loc_href', 'stadium_a_href', 'stadium_a_tag_count', 'stadium_br_tag_count', 'stadium_a_tags_matching', 'notice']\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv(\"wikipedia_clubs.csv\")\n",
    "print(len(df.columns))\n",
    "dtype_dict = {col: pl.Utf8 for col in headers}\n",
    "df_wikipedia_clubs = pl.read_csv(clubs,dtypes = dtype_dict)\n",
    "print(len(df_wikipedia_clubs.columns))\n",
    "print(df.columns)\n",
    "print(df_wikipedia_clubs.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
